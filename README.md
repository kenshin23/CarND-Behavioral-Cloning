# Project: Behavioral Cloning
[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

"Train a deep neural network to drive a car like you!"

Udacity created a simulator based on the Unity engine that uses real game physics to create a close approximation to real driving.

Download it here:

[Linux](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f0f7_simulator-linux/simulator-linux.zip)
[macOS](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f290_simulator-macos/simulator-macos.zip)
[Windows 32-bit](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f4b6_simulator-windows-32/simulator-windows-32.zip)
[Windows 64-bit](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f3a4_simulator-windows-64/simulator-windows-64.zip)

Beta Simulators

[Linux](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587527cb_udacity-sdc-udacity-self-driving-car-simulator-dominique-development-linux-desktop-64-bit-5/udacity-sdc-udacity-self-driving-car-simulator-dominique-development-linux-desktop-64-bit-5.zip)
[macOS](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587525b2_udacity-sdc-udacity-self-driving-car-simulator-dominique-default-mac-desktop-universal-5/udacity-sdc-udacity-self-driving-car-simulator-dominique-default-mac-desktop-universal-5.zip)
[Windows](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/58752736_udacity-sdc-udacity-self-driving-car-simulator-dominique-default-windows-desktop-64-bit-4/udacity-sdc-udacity-self-driving-car-simulator-dominique-default-windows-desktop-64-bit-4.zip)

# Network

Two popular models for driving a car were made by [NVIDIA]() and [comma.ai](https://github.com/commaai/research/blob/master/train_steering_model.py). In particular, NVIDIA's model is simple enough that it can be replicated in Keras with very little effort, so it seemed like a good idea to do so in this project.

It should be noted that, rather than classification (which was done in [project 2](https://github.com/kenshin23/CarND-Traffic-Sign-Classifier-Project)), this is a regression problem, where the neural network is shown an image, and the output of the network should be a steering angle, to keep the car inside the drivable portion of the track, as noted on the project's review rubric (not mentioned here for brevity.)

# Training Approach

## Simulation / Choice of training data

To train the model, the simulator was run on Windows 10 x64 using a PlayStation 4 controller to try and keep inputs smooth. Approximately 5-6 laps were run around the test (left) course, with 2 additional laps training for recovery, i.e.: one where I recorded driving going back to center from the right part of the track, and a similar one but from the left side. To avoid bias (detailed in the augmentation section below), 5-6 more laps were run around the track, but backwards.

The resulting images and CSV file were moved to a GPU-enabled Ubuntu Linux local instance, where they were to be processed.

## Pre-processing

The intention is to train the model on features of the track that should result in the model being able to correctly predict steering angles, thus keeping the car inside the track safely (instead of training on features like the hood of the car, the sky or trees, etc.) so the first pre-processing of the images was to crop the upper and lower portions of each image, so that the hood of the car and the part above the horizon were excluded.

Next, since the track features different lighting conditions, images were converted to greyscale and normalized to make sure I have a well-conditioned problem (zero mean, equal variance.)

One thing to note is that the simulator generates data from 3 cameras: front, left and right. Coupled with the steering angle and transforming the side cameras when needed, this should give me all of the data I need to train the network; do note that when run, the model only sees and recognizes the center camera image, so I needed to transform left and right side camera images as if they were taken from the center camera. This has the added advantage of simulating recovery images, so it was not entirely necessary to explicitly get them.

## Augmentation

The test track basically consists on a seemingly (varying) constant turn to the left, with a single straight and a single right turn. Training on just the dataset generated by driving on this track, would make the model biased towards driving straight or turning to the left. This makes it necessary for us to augment the samples by flipping/jittering the original images, and/or discarding large portions of the data were the driving was mostly done straight (roughly around 70% of the dataset), to reduce this particular bias.

The classroom notes for this project and [a post by Vivek Yadav](https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9) suggest not loading the whole image set into memory, as it is too large, but instead creating a generator which would process and augment the images on the fly. This has been done on this project as well, based on the functions provided.

## Training

It's time to train the model. I use a Sequential Keras model, with an Adam optimizer function. This should help with the learning rate (as it is not needed to be tuned manually) and the amount of epochs required to train. Also, I've used dropout layers to account for the fact that the straight driving is 5x larger than steering left or right; this prevents overfitting as well. As mentioned before, I'm replicating the NVIDIA model described as follows:

```____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
Input_Normalization (Lambda)     (None, 66, 200, 3)    0           lambda_input_1[0][0]             
____________________________________________________________________________________________________
Convolution_1 (Convolution2D)    (None, 31, 98, 24)    1824        Input_Normalization[0][0]        
____________________________________________________________________________________________________
Convolution_2 (Convolution2D)    (None, 14, 47, 36)    21636       Convolution_1[0][0]              
____________________________________________________________________________________________________
Convolution_3 (Convolution2D)    (None, 5, 22, 48)     43248       Convolution_2[0][0]              
____________________________________________________________________________________________________
Convolution_4 (Convolution2D)    (None, 3, 20, 64)     27712       Convolution_3[0][0]              
____________________________________________________________________________________________________
Convolution_5 (Convolution2D)    (None, 1, 18, 64)     36928       Convolution_4[0][0]              
____________________________________________________________________________________________________
Flatten (Flatten)                (None, 1152)          0           Convolution_5[0][0]              
____________________________________________________________________________________________________
Flatten_Dropout (Dropout)        (None, 1152)          0           Flatten[0][0]                    
____________________________________________________________________________________________________
Fully_Connected_0 (Dense)        (None, 1152)          1328256     Flatten_Dropout[0][0]            
____________________________________________________________________________________________________
Fully_Connected_1 (Dense)        (None, 100)           115300      Fully_Connected_0[0][0]          
____________________________________________________________________________________________________
Fully_Connected_2 (Dense)        (None, 50)            5050        Fully_Connected_1[0][0]          
____________________________________________________________________________________________________
Fully_Connected_3 (Dense)        (None, 10)            510         Fully_Connected_2[0][0]          
____________________________________________________________________________________________________
Output (Dense)                   (None, 1)             11          Fully_Connected_3[0][0]          
====================================================================================================
Total params: 1,580,475
Trainable params: 1,580,475
Non-trainable params: 0

```
The NVIDIA model doesn't specify using dropout layers, but I decided to use them to help prevent against overfitting. 

I had originally started to train the model with a fixed number of epochs, which I changed according to the perceived performance on the test track. However, and even though I made use of a GPU to train the model (which had the bonus of speeding things up dramatically) it soon proved to be very tiresome. The solution was to train the model during a set amount of epochs, and compare the validation accuracy after each one. If it was found that the accuracy lowered, then it would not be necessary to continue training the model.

