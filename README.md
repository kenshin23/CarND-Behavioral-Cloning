# Project: Behavioral Cloning
[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

Behavioral Cloning project for Udacity's Self-Driving Car Nanodegree Program

"Train a deep neural network to drive a car like you!"

Udacity created a simulator based on the Unity engine that uses real game physics to create a close approximation to real driving.

Download it here:

[Linux](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f0f7_simulator-linux/simulator-linux.zip)
[macOS](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f290_simulator-macos/simulator-macos.zip)
[Windows 32-bit](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f4b6_simulator-windows-32/simulator-windows-32.zip)
[Windows 64-bit](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f3a4_simulator-windows-64/simulator-windows-64.zip)

Beta Simulators

[Linux](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587527cb_udacity-sdc-udacity-self-driving-car-simulator-dominique-development-linux-desktop-64-bit-5/udacity-sdc-udacity-self-driving-car-simulator-dominique-development-linux-desktop-64-bit-5.zip)
[macOS](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587525b2_udacity-sdc-udacity-self-driving-car-simulator-dominique-default-mac-desktop-universal-5/udacity-sdc-udacity-self-driving-car-simulator-dominique-default-mac-desktop-universal-5.zip)
[Windows](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/58752736_udacity-sdc-udacity-self-driving-car-simulator-dominique-default-windows-desktop-64-bit-4/udacity-sdc-udacity-self-driving-car-simulator-dominique-default-windows-desktop-64-bit-4.zip)

# Network

Two popular models for driving a car were made by [NVIDIA]() and [comma.ai](https://github.com/commaai/research/blob/master/train_steering_model.py). In particular, NVIDIA's model is simple enough that it can be replicated in Keras with very little effort, so it seemed like a good idea to do so in this project.

It should be noted that, rather than classification (which was done in [project 2](https://github.com/kenshin23/CarND-Traffic-Sign-Classifier-Project)), this is a regression problem, where the neural network is shown an image, and the output of the network should be a steering angle, to keep the car inside the drivable portion of the track, as noted on the project's review rubric (not mentioned here for brevity.)

# Training Approach

## Simulation / Choice of training data

To train the model, the simulator was run on Windows 10 x64 using a PlayStation 4 controller to keep inputs smooth. Approximately 5-6 laps were run around the test (left) course, with 2 additional laps training for recovery, i.e.: one where I recorded driving going back to center from the right part of the track, and a similar one but from the left side. To avoid bias (detailed in the augmentation section below), 5-6 more laps were run around the track, but backwards.

The resulting images and CSV file were moved to a GPU-enabled Ubuntu Linux local instance, where they were to be processed.

## Preprocessing

The intention is to train the model on features of the track that should result in the model being able to correctly predict steering angles, thus keeping the car inside the track safely (instead of training on features like the hood of the car, the sky or trees, etc.) so the first preprocessing of the images was to crop the upper and lower portions of each image, so that the hood of the car and the part above the horizon were excluded.

Next, since the track features different lighting conditions, images were converted to grayscale and normalized to make sure we have a well-conditioned problem (zero mean, equal variance.)

One thing to note is that the simulator generates data from 3 cameras: front, left and right. Coupled with the steering angle and transforming the side cameras when needed, this should give us all of the data we need to train the network; do note that when run, the model only sees and recognizes the center camera image, so we need to transform left and right side camera images as if they were taken from the center camera.

## Augmentation

The test track basically consists on a seemingly (varying) constant turn to the left, with a single straight and a single right turn. Training on just the dataset generated by driving on this track, would make the model biased towards driving straight or turning to the left. This makes it necessary for us to augment the samples by flipping/jittering the original images, and/or discarting large portions of the data were the driving was mostly done straight (roughly around 70% of the dataset), to reduce this particular bias.

The classroom notes for this project and [a post by Vivek Yadak](https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9) suggest not loading the whole image set into memory, as it is too large, but instead creating a generator which would process and augment the images on the fly. This has been done on this project as well, based on the functions provided.

## Training

It's time to train the model. We use a Sequential Keras model, with an Adam optimizer function. This should help with the learning rate and the amount of epochs required to train. Also, we've used dropout layers to account for the fact that the straight driving is 5x larger than steering left or right; this prevents overfitting as well. As mentioned before, we're replicating the NVIDIA model described as follows:
